{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_cartpole.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMI07a+WWoBK/31r4FrAsE3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"wJYbLW_nWE7A"},"source":["# Do this and follow instructions for render to work\n","\n","#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-3N4bGDhcZV","executionInfo":{"status":"ok","timestamp":1618434599631,"user_tz":240,"elapsed":11306,"user":{"displayName":"Pranjal Gupta","photoUrl":"","userId":"09893061847420234743"}},"outputId":"5d26c942-a585-4fed-8017-c3a36c3afb07"},"source":["!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install gym[atari] > /dev/null 2>&1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (56.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RG53aZtuhgUy","executionInfo":{"status":"ok","timestamp":1618805003269,"user_tz":240,"elapsed":3137,"user":{"displayName":"Pranjal Gupta","photoUrl":"","userId":"09893061847420234743"}}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","import cv2\n","\n","from IPython import display as ipythondisplay"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lOUSUcythitK","executionInfo":{"status":"ok","timestamp":1618434606839,"user_tz":240,"elapsed":1179,"user":{"displayName":"Pranjal Gupta","photoUrl":"","userId":"09893061847420234743"}},"outputId":"b6efed30-3838-4a43-f56d-13c1c94b50be"},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f0299f9f890>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"8Ydo1pS9hmnE"},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"ukWOviHthpMc","executionInfo":{"status":"ok","timestamp":1618434624571,"user_tz":240,"elapsed":1521,"user":{"displayName":"Pranjal Gupta","photoUrl":"","userId":"09893061847420234743"}},"outputId":"1fee4e58-5c7a-4d8a-cc6f-ffbc24fadcfb"},"source":["# Check if render indeed works by plotting initial state\n","import matplotlib.pyplot as plt\n","\n","env = gym.make(\"CartPole-v1\")\n","env.reset()\n","\n","def plot_initial(env):\n","  img = env.render(mode=\"rgb_array\")\n","  np_img = np.array(img)\n","\n","  print('Shape of image: ', np_img.shape)\n","\n","  plt.imshow(np_img)\n","  plt.show()\n","\n","plot_initial(env)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of image:  (400, 600, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUuogFCSh1kl","executionInfo":{"status":"ok","timestamp":1618434632522,"user_tz":240,"elapsed":337,"user":{"displayName":"Pranjal Gupta","photoUrl":"","userId":"09893061847420234743"}},"outputId":"2e6cb2d0-6e82-40c0-f409-abfd96d67b4c"},"source":["# Store frames resulting from random actions\n","def store_frames(env):\n","  \n","  epochs = 0\n","\n","  frames = [] # for animation\n","\n","  env.reset()\n","\n","  done = False\n","\n","  while not done:\n","\n","    action = env.action_space.sample()\n","    state, reward, done, info = env.step(action)\n","    \n","    # Put each rendered frame into dict for animation\n","    img_frame = np.array(env.render(mode=\"rgb_array\"))\n","    frames.append(img_frame)\n","    epochs += 1\n","\n","  print(\"Timesteps taken: {}\".format(epochs))\n","\n","  return frames\n","\n","frames = store_frames(env)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Timesteps taken: 31\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n-AjJrqhlRBW"},"source":["# Make a video sequence from stored frames of the random agent\n","def make_video(frames):\n","\n","  size = frames[0].shape # Size of each image in frame\n","  fps = 12\n","  out = cv2.VideoWriter('random_agent.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (size[1], size[0])) # Assumes RGB, need to only pass (width, height)\n","\n","  for frame in frames:\n","      data = frame[:, :, ::-1] # assumes BGR format, so reverse channels\n","      out.write(data)\n","\n","  out.release()\n","\n","make_video(frames)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"du0cVAktr7k2"},"source":["# CNN Implementation, using class\n","\n","# Import packages\n","import numpy as np\n","import gym\n","\n","import time\n","import random\n","from collections import deque\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0zPEKekNHVs"},"source":["def CNN_agent(image_size, num_actions, dueling):\n","  # Architecture of the CNN, accepts a stack of 4 images(current state) and returns q value corresponding to that state\n","  # Inputs:\n","  # image_size: Size of the input image frames\n","  # num_actions: Number of actions\n","  # Outputs:\n","  # Compiled Architecture of the CNN\n","\n","  # My own architecture, feel free to modify to the actual implementation\n","  learning_rate = 0.001\n","\n","  input_tensor = keras.Input(shape=image_size)\n","\n","  main_stream = Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(input_tensor)\n","  main_stream = Conv2D(filters = 32, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(main_stream)\n","  main_stream = MaxPooling2D((2,2))(main_stream)\n","\n","  main_stream = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(main_stream)\n","  main_stream = Conv2D(filters = 64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(main_stream)\n","  main_stream = MaxPooling2D((2,2))(main_stream)\n","\n","  main_stream = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(main_stream)\n","  main_stream = Conv2D(filters = 128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(main_stream)\n","  main_stream = MaxPooling2D((2,2))(main_stream)\n","\n","  main_stream = Conv2D(filters=128, kernel_size=(5,5), strides=2, padding=\"same\", activation=\"relu\")(main_stream)\n","  main_stream = Dropout(rate=0.2)(main_stream)\n","\n","  main_stream = Flatten()(main_stream)\n","\n","  main_stream = Dense(units=256, activation=\"relu\", kernel_initializer=\"he_uniform\")(main_stream)\n","  main_stream = Dense(units=128, activation=\"relu\", kernel_initializer=\"he_uniform\")(main_stream)\n","\n","  if dueling: # Add value and advantage stream if the dueling flag is set to true\n","\n","    value_stream = Dense(units=64, activation=\"relu\", kernel_initializer=\"he_uniform\")(main_stream)\n","    V = Dense(units=1, activation=\"linear\", kernel_initializer=\"he_uniform\")(value_stream)\n","\n","    advantage_stream = Dense(units=64, activation=\"relu\", kernel_initializer=\"he_uniform\")(main_stream)\n","    A = Dense(units=num_actions, activation=\"linear\", kernel_initializer=\"he_uniform\")(advantage_stream)\n","\n","    out_tensor = V + tf.subtract(A, tf.reduce_mean(A, axis=1, keepdims=True))\n","\n","  else:\n","\n","    out_tensor = Dense(units=num_actions, activation=\"linear\", kernel_initializer=\"he_uniform\")(main_stream)\n","\n","  model = keras.Model(inputs=input_tensor, outputs=out_tensor)\n","\n","  model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[tf.keras.metrics.MeanSquaredError()])\n","\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oqac17W5DK2z","outputId":"9324863c-0da3-4c94-ef22-16e4694089ad"},"source":["class CNN_DQN_agent:\n","\n","  def __init__(self, env_name):\n","    # Parameters\n","    # env_name: Name of the game we are playing\n","\n","    self.game_name = env_name # Name of the game we are playing\n","    self.game = gym.make(env_name) # Create the game environment\n","\n","    self.double_dqn = False # flag for Double DQN learning (slows down a lot)\n","    self.dueling = False # Flag for using dueling architecture\n","\n","    self.min_epsilon = 0.01 # At the minimum we explore 1 % of the time\n","    self.max_epsilon = 1 # Start with 100% exploration\n","    self.epsilon = self.max_epsilon # Start with max exploration and decay exponentially with # of episodes\n","    self.decay = 0.01 # decay rate (exponential)\n","\n","    # We reduce our original RGB rendered image to a grayscale image of the size given below\n","    self.img_row = 160 \n","    self.img_col = 210\n","    self.num_frames = 4 # Number of frames to stack\n","\n","    self.image_frames = np.zeros((self.img_row, self.img_col, self.num_frames)) # Stores the current stack of 4 frames (refered to as image state)\n","\n","    self.image_size = (self.img_row, self.img_col, self.num_frames) # Create tuple of image state size\n","    self.num_actions = self.game.action_space.n # Number of actions\n","\n","    # Replay memory\n","    self.memory_len = 50000 \n","    self.replay_memory = deque(maxlen=self.memory_len)\n","\n","    self.gamma = 0.618 # Discount factor\n","    self.alpha = 0.7 # Learning rate of Q-learning\n","\n","    self.batch_size = 8 # Since its images, use a smaller batchsize to train\n","\n","    # Initialize networks\n","    self.main_net = CNN_agent(self.image_size, self.num_actions, self.dueling)\n","    self.target_net = CNN_agent(self.image_size, self.num_actions, self.dueling)\n","    self.target_net.set_weights(self.main_net.get_weights()) \n","\n","    self.train_episodes = 3000\n","\n","    self.num_steps_main = 4\n","    self.num_update_target = 100\n","\n","    self.steps = 0\n","\n","    self.all_rewards = []\n","\n","  # Function that converts the original RGB rendered image of the current state (which is 400x600x3) to self.image_size\n","  def render_image(self):\n","\n","    cur_img_state = self.game.render(mode=\"rgb_array\") # Render current state of the game environment\n","    cur_img_gray = cv2.cvtColor(cur_img_state, cv2.COLOR_RGB2GRAY) # RGB to gray using cv2\n","    cur_img_resized = cv2.resize(cur_img_gray, (self.img_col, self.img_row), interpolation=cv2.INTER_CUBIC) # resize uses (img_col, img_row)\n","\n","    self.image_frames = np.roll(self.image_frames, shift=1, axis=2) # A circular shift of 1 applied along the frame axis\n","    self.image_frames[:, :, 0] = cur_img_resized # Overwrite the oldest frame with the current resized image\n","\n","    return self.image_frames\n","\n","  # Add batch axis to make predictions for single element states\n","  def add_batch_axis(self, cur_img_state):\n","    \n","    cur_img_expanded = np.expand_dims(cur_img_state, axis=0)\n","    \n","    return cur_img_expanded\n","\n","  # Select action to take by argmax over current Q-value as predicted by main_net (Exploitation)\n","  def select_action(self, cur_img_state):\n","\n","    cur_img_expanded = self.add_batch_axis(cur_img_state)\n","    q_val_cur = np.squeeze(self.main_net.predict(cur_img_expanded))\n","    selected_action = np.argmax(q_val_cur)\n","\n","    return selected_action\n","\n","  # Train the main net (The only difference here is that our states are stacked frames)\n","  def train_main_net(self):\n","\n","    min_experience = 1000\n","\n","    if len(self.replay_memory) < min_experience:\n","      \n","      return\n","\n","    num_experiences = 128\n","\n","    random_experiences = random.sample(self.replay_memory, num_experiences)\n","    cur_img_states = np.array([experience[0] for experience in random_experiences])\n","    next_img_states = np.array([experience[3] for experience in random_experiences])\n","\n","    q_vals_cur = self.main_net.predict(cur_img_states)\n","    q_vals_next = self.target_net.predict(next_img_states)\n","\n","    img_states = []\n","    target_q_vals = []\n","\n","    for i, (state, action, reward, next_state, done) in enumerate(random_experiences):\n","\n","      if done:\n","\n","        q_cur_state = reward\n","\n","      else:\n","\n","        if self.double_dqn:\n","\n","          expand_next_state = add_batch_axis(next_state)\n","          sel_action_main = np.argmax(np.squeeze(self.main_net.predict(expand_next_state)))\n","\n","          q_val_target = q_vals_next[i]\n","\n","          q_cur_state = reward + self.gamma*q_val_target[sel_action_main]\n","\n","        else:\n","\n","          q_cur_state = reward + self.gamma*np.max(q_vals_next[i])\n","\n","      q_val_cur = q_vals_cur[i]\n","\n","      if self.double_dqn:\n","\n","        q_val_cur[action] = q_cur_state\n","\n","      else:\n","\n","        q_val_cur[action] = (1-self.alpha)*q_val_cur[action] + self.alpha*q_cur_state\n","\n","      img_states.append(state)\n","      target_q_vals.append(q_val_cur)\n","    \n","    X = np.array(img_states)\n","    y = np.array(target_q_vals)\n","\n","    self.main_net.fit(X, y, batch_size=self.batch_size, shuffle=True)\n","\n","  # Trains the agent over a specified number of episodes\n","  def train_on_game(self):\n","\n","    for eps in range(self.train_episodes):\n","\n","      cur_state = self.game.reset() # Reset environment \n","      cur_img_state = self.render_image() # Add image of initial state to stack of frames\n","\n","      episode_reward = 0\n","      \n","      done = False \n","\n","      while not done:\n","\n","        u = np.random.uniform(0, 1)\n","\n","        if u < self.epsilon: # Explore\n","\n","          cur_action = self.game.action_space.sample()\n","\n","        else: # Exploit\n","\n","          cur_action = self.select_action(cur_img_state)\n","\n","        next_state, reward, done, info = self.game.step(cur_action)\n","        next_img_state = self.render_image() # Add image of the next state to the stack of frames\n","\n","        self.replay_memory.append([cur_img_state, cur_action, reward, next_img_state, done])\n","\n","        self.steps += 1\n","        episode_reward += reward\n","\n","        if self.steps % self.num_steps_main == 0 or done:\n","\n","          self.train_main_net()\n","\n","        if self.steps % self.num_update_target == 0:\n","          \n","          print('Copying weights from main to target')\n","          self.target_net.set_weights(self.main_net.get_weights())\n","\n","        cur_img_state = next_img_state\n","\n","      if self.steps % evaluate_steps == 0:\n","\n","        # 5 times\n","        # Evaluate by using greedy actions\n","        # store each frame\n","        # \n","\n","      self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay*eps)\n","\n","      # Print some statistics to check if there's learning\n","      print('Epsiode {} statistics: '.format(eps+1))\n","      print('Reward obtained: ', episode_reward)\n","\n","      self.all_rewards.append(episode_reward)\n","\n","\n","    \n","env_name = \"CartPole-v1\"\n","agent = CNN_DQN_agent(env_name)\n","agent.train_on_game()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epsiode 1 statistics: \n","Reward obtained:  17.0\n","Epsiode 2 statistics: \n","Reward obtained:  12.0\n","Epsiode 3 statistics: \n","Reward obtained:  40.0\n","Epsiode 4 statistics: \n","Reward obtained:  14.0\n","Copying weights from main to target\n","Epsiode 5 statistics: \n","Reward obtained:  22.0\n","Epsiode 6 statistics: \n","Reward obtained:  14.0\n","Epsiode 7 statistics: \n","Reward obtained:  12.0\n","Epsiode 8 statistics: \n","Reward obtained:  12.0\n","Epsiode 9 statistics: \n","Reward obtained:  19.0\n","Epsiode 10 statistics: \n","Reward obtained:  23.0\n","Copying weights from main to target\n","Epsiode 11 statistics: \n","Reward obtained:  16.0\n","Epsiode 12 statistics: \n","Reward obtained:  10.0\n","Epsiode 13 statistics: \n","Reward obtained:  14.0\n","Epsiode 14 statistics: \n","Reward obtained:  18.0\n","Epsiode 15 statistics: \n","Reward obtained:  36.0\n","Copying weights from main to target\n","Epsiode 16 statistics: \n","Reward obtained:  32.0\n","Epsiode 17 statistics: \n","Reward obtained:  22.0\n","Epsiode 18 statistics: \n","Reward obtained:  13.0\n","Epsiode 19 statistics: \n","Reward obtained:  12.0\n","Epsiode 20 statistics: \n","Reward obtained:  38.0\n","Copying weights from main to target\n","Epsiode 21 statistics: \n","Reward obtained:  30.0\n","Epsiode 22 statistics: \n","Reward obtained:  13.0\n","Epsiode 23 statistics: \n","Reward obtained:  55.0\n","Copying weights from main to target\n","Epsiode 24 statistics: \n","Reward obtained:  32.0\n","Epsiode 25 statistics: \n","Reward obtained:  22.0\n","Epsiode 26 statistics: \n","Reward obtained:  19.0\n","Copying weights from main to target\n","Epsiode 27 statistics: \n","Reward obtained:  41.0\n","Epsiode 28 statistics: \n","Reward obtained:  11.0\n","Epsiode 29 statistics: \n","Reward obtained:  16.0\n","Epsiode 30 statistics: \n","Reward obtained:  12.0\n","Epsiode 31 statistics: \n","Reward obtained:  23.0\n","Epsiode 32 statistics: \n","Reward obtained:  17.0\n","Copying weights from main to target\n","Epsiode 33 statistics: \n","Reward obtained:  34.0\n","Epsiode 34 statistics: \n","Reward obtained:  13.0\n","Epsiode 35 statistics: \n","Reward obtained:  11.0\n","Epsiode 36 statistics: \n","Reward obtained:  27.0\n","Epsiode 37 statistics: \n","Reward obtained:  12.0\n","Copying weights from main to target\n","Epsiode 38 statistics: \n","Reward obtained:  29.0\n","Epsiode 39 statistics: \n","Reward obtained:  13.0\n","Epsiode 40 statistics: \n","Reward obtained:  8.0\n","Epsiode 41 statistics: \n","Reward obtained:  13.0\n","Epsiode 42 statistics: \n","Reward obtained:  22.0\n","Epsiode 43 statistics: \n","Reward obtained:  18.0\n","Copying weights from main to target\n","Epsiode 44 statistics: \n","Reward obtained:  13.0\n","Epsiode 45 statistics: \n","Reward obtained:  22.0\n","Epsiode 46 statistics: \n","Reward obtained:  13.0\n","Epsiode 47 statistics: \n","Reward obtained:  17.0\n","Epsiode 48 statistics: \n","Reward obtained:  11.0\n","Epsiode 49 statistics: \n","Reward obtained:  18.0\n","Epsiode 50 statistics: \n","Reward obtained:  15.0\n","16/16 [==============================] - 2s 22ms/step - loss: 21.4496 - mean_squared_error: 2360.3438\n","Copying weights from main to target\n","16/16 [==============================] - 0s 23ms/step - loss: 1.0648 - mean_squared_error: 3.5459\n","16/16 [==============================] - 0s 22ms/step - loss: 0.6202 - mean_squared_error: 1.8040\n","16/16 [==============================] - 0s 22ms/step - loss: 0.6838 - mean_squared_error: 1.9190\n","16/16 [==============================] - 0s 22ms/step - loss: 0.2871 - mean_squared_error: 0.6929\n","16/16 [==============================] - 0s 22ms/step - loss: 0.1628 - mean_squared_error: 0.3659\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0857 - mean_squared_error: 0.1861\n","Epsiode 51 statistics: \n","Reward obtained:  25.0\n","16/16 [==============================] - 0s 22ms/step - loss: 0.1113 - mean_squared_error: 0.2598\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0710 - mean_squared_error: 0.1711\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0302 - mean_squared_error: 0.0700\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0433 - mean_squared_error: 0.1008\n","Epsiode 52 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0343 - mean_squared_error: 0.0810\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0253 - mean_squared_error: 0.0603\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0289 - mean_squared_error: 0.0678\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0298 - mean_squared_error: 0.0694\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0248 - mean_squared_error: 0.0570\n","Epsiode 53 statistics: \n","Reward obtained:  15.0\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0220 - mean_squared_error: 0.0502\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0473 - mean_squared_error: 0.1083\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0109 - mean_squared_error: 0.0253\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0267 - mean_squared_error: 0.0631\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0302 - mean_squared_error: 0.0715\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0329 - mean_squared_error: 0.0768\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0323 - mean_squared_error: 0.0750\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0499 - mean_squared_error: 0.1174\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0318 - mean_squared_error: 0.0739\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0341 - mean_squared_error: 0.0758\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0249 - mean_squared_error: 0.0554\n","Epsiode 54 statistics: \n","Reward obtained:  41.0\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0280 - mean_squared_error: 0.0641\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0191 - mean_squared_error: 0.0439\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0372 - mean_squared_error: 0.0840\n","Copying weights from main to target\n","16/16 [==============================] - 0s 23ms/step - loss: 0.1829 - mean_squared_error: 0.3666\n","Epsiode 55 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0827 - mean_squared_error: 0.1654\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0229 - mean_squared_error: 0.0459\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0148 - mean_squared_error: 0.0295\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0136 - mean_squared_error: 0.0271\n","Epsiode 56 statistics: \n","Reward obtained:  15.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0070 - mean_squared_error: 0.0140\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0081 - mean_squared_error: 0.0162\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0059 - mean_squared_error: 0.0118\n","Epsiode 57 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0104 - mean_squared_error: 0.0208\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0042 - mean_squared_error: 0.0085\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0021 - mean_squared_error: 0.0042\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0034 - mean_squared_error: 0.0069\n","Epsiode 58 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0027 - mean_squared_error: 0.0053\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0035 - mean_squared_error: 0.0071\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0040 - mean_squared_error: 0.0080\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0048 - mean_squared_error: 0.0096\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0021 - mean_squared_error: 0.0042\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0022 - mean_squared_error: 0.0044\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0033 - mean_squared_error: 0.0067\n","16/16 [==============================] - 0s 22ms/step - loss: 0.0039 - mean_squared_error: 0.0077\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0037 - mean_squared_error: 0.0073\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0025 - mean_squared_error: 0.0050\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0017 - mean_squared_error: 0.0035\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0017 - mean_squared_error: 0.0035\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0026 - mean_squared_error: 0.0052\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0027 - mean_squared_error: 0.0054\n","Epsiode 59 statistics: \n","Reward obtained:  51.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0042 - mean_squared_error: 0.0085\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0041 - mean_squared_error: 0.0081\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0025 - mean_squared_error: 0.0049\n","Copying weights from main to target\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0615 - mean_squared_error: 0.1229\n","Epsiode 60 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0365 - mean_squared_error: 0.0731\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0200 - mean_squared_error: 0.0401\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0121 - mean_squared_error: 0.0242\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0079 - mean_squared_error: 0.0158\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0062 - mean_squared_error: 0.0124\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0165 - mean_squared_error: 0.0483\n","Epsiode 61 statistics: \n","Reward obtained:  20.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0087 - mean_squared_error: 0.0173\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0034 - mean_squared_error: 0.0067\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0020 - mean_squared_error: 0.0041\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0020 - mean_squared_error: 0.0040\n","Epsiode 62 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0014 - mean_squared_error: 0.0028\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0011 - mean_squared_error: 0.0023\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0015 - mean_squared_error: 0.0030\n","Epsiode 63 statistics: \n","Reward obtained:  12.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0013 - mean_squared_error: 0.0025\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0013 - mean_squared_error: 0.0026\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0011 - mean_squared_error: 0.0021\n","Epsiode 64 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0010 - mean_squared_error: 0.0021\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0016 - mean_squared_error: 0.0033\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0013 - mean_squared_error: 0.0026\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0010 - mean_squared_error: 0.0020\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0041 - mean_squared_error: 0.0083\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0023 - mean_squared_error: 0.0046\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0059 - mean_squared_error: 0.0127\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0017 - mean_squared_error: 0.0033\n","Epsiode 65 statistics: \n","Reward obtained:  29.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0011 - mean_squared_error: 0.0022\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0012 - mean_squared_error: 0.0023\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0015 - mean_squared_error: 0.0030\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0013 - mean_squared_error: 0.0025\n","Copying weights from main to target\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0280 - mean_squared_error: 0.0559\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0123 - mean_squared_error: 0.0246\n","Epsiode 66 statistics: \n","Reward obtained:  18.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0106 - mean_squared_error: 0.0212\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0081 - mean_squared_error: 0.0161\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0097 - mean_squared_error: 0.0195\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0065 - mean_squared_error: 0.0129\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0052 - mean_squared_error: 0.0103\n","Epsiode 67 statistics: \n","Reward obtained:  17.0\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0051 - mean_squared_error: 0.0101\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0075 - mean_squared_error: 0.0150\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0046 - mean_squared_error: 0.0092\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0070 - mean_squared_error: 0.0139\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0076 - mean_squared_error: 0.0151\n","Epsiode 68 statistics: \n","Reward obtained:  17.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0044 - mean_squared_error: 0.0088\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0056 - mean_squared_error: 0.0111\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0088 - mean_squared_error: 0.0176\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0067 - mean_squared_error: 0.0134\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0043 - mean_squared_error: 0.0086\n","Epsiode 69 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0049 - mean_squared_error: 0.0097\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0050 - mean_squared_error: 0.0100\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0064 - mean_squared_error: 0.0129\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0032 - mean_squared_error: 0.0064\n","Epsiode 70 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0026 - mean_squared_error: 0.0052\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0048 - mean_squared_error: 0.0097\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0066 - mean_squared_error: 0.0133\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0062 - mean_squared_error: 0.0125\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0088 - mean_squared_error: 0.0177\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0049 - mean_squared_error: 0.0098\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0063 - mean_squared_error: 0.0125\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0049 - mean_squared_error: 0.0098\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0040 - mean_squared_error: 0.0080\n","Copying weights from main to target\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0223 - mean_squared_error: 0.0446\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0126 - mean_squared_error: 0.0251\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0055 - mean_squared_error: 0.0110\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0064 - mean_squared_error: 0.0127\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0089 - mean_squared_error: 0.0178\n","Epsiode 71 statistics: \n","Reward obtained:  53.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0109 - mean_squared_error: 0.0219\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0066 - mean_squared_error: 0.0132\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0120 - mean_squared_error: 0.0240\n","Epsiode 72 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0061 - mean_squared_error: 0.0121\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0073 - mean_squared_error: 0.0146\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0088 - mean_squared_error: 0.0175\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0061 - mean_squared_error: 0.0122\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0086 - mean_squared_error: 0.0172\n","Epsiode 73 statistics: \n","Reward obtained:  17.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0077 - mean_squared_error: 0.0155\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0050 - mean_squared_error: 0.0099\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0054 - mean_squared_error: 0.0108\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0089 - mean_squared_error: 0.0179\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0069 - mean_squared_error: 0.0137\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0107 - mean_squared_error: 0.0214\n","Epsiode 74 statistics: \n","Reward obtained:  23.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0069 - mean_squared_error: 0.0138\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0078 - mean_squared_error: 0.0157\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0065 - mean_squared_error: 0.0129\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0086 - mean_squared_error: 0.0172\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0055 - mean_squared_error: 0.0110\n","Epsiode 75 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0040 - mean_squared_error: 0.0080\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0080 - mean_squared_error: 0.0160\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0058 - mean_squared_error: 0.0116\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0062 - mean_squared_error: 0.0123\n","Epsiode 76 statistics: \n","Reward obtained:  12.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0052 - mean_squared_error: 0.0104\n","Copying weights from main to target\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0126 - mean_squared_error: 0.0251\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0132 - mean_squared_error: 0.0264\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0192 - mean_squared_error: 0.0383\n","16/16 [==============================] - 0s 25ms/step - loss: 0.2053 - mean_squared_error: 0.4255\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0809 - mean_squared_error: 0.1619\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0828 - mean_squared_error: 0.1657\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0283 - mean_squared_error: 0.0566\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0246 - mean_squared_error: 0.0492\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0133 - mean_squared_error: 0.0267\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0122 - mean_squared_error: 0.0244\n","Epsiode 77 statistics: \n","Reward obtained:  42.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0119 - mean_squared_error: 0.0237\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0111 - mean_squared_error: 0.0223\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0067 - mean_squared_error: 0.0133\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0044 - mean_squared_error: 0.0088\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0077 - mean_squared_error: 0.0154\n","Epsiode 78 statistics: \n","Reward obtained:  15.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0054 - mean_squared_error: 0.0109\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0097 - mean_squared_error: 0.0195\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0100 - mean_squared_error: 0.0201\n","Epsiode 79 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0139 - mean_squared_error: 0.0278\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0100 - mean_squared_error: 0.0200\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0066 - mean_squared_error: 0.0132\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0075 - mean_squared_error: 0.0150\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_squared_error: 0.0169\n","Epsiode 80 statistics: \n","Reward obtained:  17.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0072 - mean_squared_error: 0.0144\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0086 - mean_squared_error: 0.0171\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0042 - mean_squared_error: 0.0084\n","Epsiode 81 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0054 - mean_squared_error: 0.0108\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0054 - mean_squared_error: 0.0107\n","Copying weights from main to target\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_squared_error: 0.0170\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0117 - mean_squared_error: 0.0235\n","Epsiode 82 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0158 - mean_squared_error: 0.0317\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0105 - mean_squared_error: 0.0209\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0093 - mean_squared_error: 0.0186\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0060 - mean_squared_error: 0.0119\n","Epsiode 83 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0097 - mean_squared_error: 0.0194\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0090 - mean_squared_error: 0.0179\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0098 - mean_squared_error: 0.0197\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0061 - mean_squared_error: 0.0122\n","Epsiode 84 statistics: \n","Reward obtained:  12.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0106 - mean_squared_error: 0.0213\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0075 - mean_squared_error: 0.0150\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0100 - mean_squared_error: 0.0199\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0077 - mean_squared_error: 0.0154\n","Epsiode 85 statistics: \n","Reward obtained:  12.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0095 - mean_squared_error: 0.0190\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0103 - mean_squared_error: 0.0206\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0123 - mean_squared_error: 0.0246\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0061 - mean_squared_error: 0.0123\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0113 - mean_squared_error: 0.0226\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0084 - mean_squared_error: 0.0167\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0072 - mean_squared_error: 0.0145\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0066 - mean_squared_error: 0.0132\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0108 - mean_squared_error: 0.0216\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0075 - mean_squared_error: 0.0149\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0075 - mean_squared_error: 0.0150\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0052 - mean_squared_error: 0.0104\n","Epsiode 86 statistics: \n","Reward obtained:  46.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0109 - mean_squared_error: 0.0218\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0078 - mean_squared_error: 0.0156\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0093 - mean_squared_error: 0.0186\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0069 - mean_squared_error: 0.0137\n","Epsiode 87 statistics: \n","Reward obtained:  12.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0123 - mean_squared_error: 0.0246\n","Copying weights from main to target\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0059 - mean_squared_error: 0.0119\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0076 - mean_squared_error: 0.0151\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_squared_error: 0.0170\n","Epsiode 88 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0102 - mean_squared_error: 0.0204\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0084 - mean_squared_error: 0.0168\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0099 - mean_squared_error: 0.0197\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0096 - mean_squared_error: 0.0192\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0069 - mean_squared_error: 0.0138\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0106 - mean_squared_error: 0.0212\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0111 - mean_squared_error: 0.0222\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0118 - mean_squared_error: 0.0235\n","Epsiode 89 statistics: \n","Reward obtained:  31.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0070 - mean_squared_error: 0.0139\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0148 - mean_squared_error: 0.0296\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0098 - mean_squared_error: 0.0196\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0085 - mean_squared_error: 0.0170\n","Epsiode 90 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0073 - mean_squared_error: 0.0146\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0042 - mean_squared_error: 0.0085\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0083 - mean_squared_error: 0.0167\n","Epsiode 91 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0022 - mean_squared_error: 0.0043\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0052 - mean_squared_error: 0.0103\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0038 - mean_squared_error: 0.0076\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0080 - mean_squared_error: 0.0160\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0087 - mean_squared_error: 0.0175\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0062 - mean_squared_error: 0.0125\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0091 - mean_squared_error: 0.0182\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0069 - mean_squared_error: 0.0137\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0158 - mean_squared_error: 0.0316\n","Copying weights from main to target\n","Epsiode 92 statistics: \n","Reward obtained:  36.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0066 - mean_squared_error: 0.0132\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0073 - mean_squared_error: 0.0145\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0091 - mean_squared_error: 0.0183\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0103 - mean_squared_error: 0.0205\n","Epsiode 93 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0089 - mean_squared_error: 0.0179\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0067 - mean_squared_error: 0.0133\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0129 - mean_squared_error: 0.0257\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0071 - mean_squared_error: 0.0141\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0081 - mean_squared_error: 0.0162\n","Epsiode 94 statistics: \n","Reward obtained:  16.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0090 - mean_squared_error: 0.0181\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0112 - mean_squared_error: 0.0224\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0147 - mean_squared_error: 0.0295\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0067 - mean_squared_error: 0.0134\n","Epsiode 95 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0096 - mean_squared_error: 0.0192\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0084 - mean_squared_error: 0.0168\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0054 - mean_squared_error: 0.0108\n","Epsiode 96 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0092 - mean_squared_error: 0.0183\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0105 - mean_squared_error: 0.0210\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0107 - mean_squared_error: 0.0214\n","Epsiode 97 statistics: \n","Reward obtained:  9.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0128 - mean_squared_error: 0.0257\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0089 - mean_squared_error: 0.0178\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0111 - mean_squared_error: 0.0221\n","Epsiode 98 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0089 - mean_squared_error: 0.0179\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0090 - mean_squared_error: 0.0181\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0093 - mean_squared_error: 0.0186\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0101 - mean_squared_error: 0.0202\n","Epsiode 99 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0090 - mean_squared_error: 0.0181\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0198 - mean_squared_error: 0.0399\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0089 - mean_squared_error: 0.0177\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0042 - mean_squared_error: 0.0083\n","Copying weights from main to target\n","Epsiode 100 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0042 - mean_squared_error: 0.0085\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0068 - mean_squared_error: 0.0136\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0157 - mean_squared_error: 0.0314\n","Epsiode 101 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0076 - mean_squared_error: 0.0152\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0127 - mean_squared_error: 0.0254\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0113 - mean_squared_error: 0.0226\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0084 - mean_squared_error: 0.0168\n","Epsiode 102 statistics: \n","Reward obtained:  12.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0107 - mean_squared_error: 0.0214\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0067 - mean_squared_error: 0.0134\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0109 - mean_squared_error: 0.0218\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0081 - mean_squared_error: 0.0162\n","Epsiode 103 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0080 - mean_squared_error: 0.0160\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0064 - mean_squared_error: 0.0128\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0139 - mean_squared_error: 0.0278\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0109 - mean_squared_error: 0.0218\n","Epsiode 104 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0095 - mean_squared_error: 0.0189\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0070 - mean_squared_error: 0.0139\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0095 - mean_squared_error: 0.0191\n","Epsiode 105 statistics: \n","Reward obtained:  9.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0123 - mean_squared_error: 0.0248\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0090 - mean_squared_error: 0.0180\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0065 - mean_squared_error: 0.0130\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0104 - mean_squared_error: 0.0209\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0114 - mean_squared_error: 0.0229\n","Epsiode 106 statistics: \n","Reward obtained:  15.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0057 - mean_squared_error: 0.0113\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0133 - mean_squared_error: 0.0266\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0141 - mean_squared_error: 0.0282\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0134 - mean_squared_error: 0.0269\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0092 - mean_squared_error: 0.0183\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0139 - mean_squared_error: 0.0279\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0110 - mean_squared_error: 0.0220\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0100 - mean_squared_error: 0.0200\n","Epsiode 107 statistics: \n","Reward obtained:  30.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0088 - mean_squared_error: 0.0176\n","Copying weights from main to target\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0159 - mean_squared_error: 0.0320\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0101 - mean_squared_error: 0.0202\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0072 - mean_squared_error: 0.0144\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0054 - mean_squared_error: 0.0108\n","Epsiode 108 statistics: \n","Reward obtained:  16.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0098 - mean_squared_error: 0.0197\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0101 - mean_squared_error: 0.0203\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0078 - mean_squared_error: 0.0156\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0081 - mean_squared_error: 0.0162\n","Epsiode 109 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0118 - mean_squared_error: 0.0237\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0102 - mean_squared_error: 0.0204\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0116 - mean_squared_error: 0.0232\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0122 - mean_squared_error: 0.0244\n","Epsiode 110 statistics: \n","Reward obtained:  15.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0071 - mean_squared_error: 0.0143\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0103 - mean_squared_error: 0.0205\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0092 - mean_squared_error: 0.0184\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0134 - mean_squared_error: 0.0267\n","Epsiode 111 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0076 - mean_squared_error: 0.0153\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0092 - mean_squared_error: 0.0184\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0085 - mean_squared_error: 0.0170\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0107 - mean_squared_error: 0.0213\n","Epsiode 112 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0097 - mean_squared_error: 0.0195\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0105 - mean_squared_error: 0.0211\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0068 - mean_squared_error: 0.0137\n","Epsiode 113 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0081 - mean_squared_error: 0.0163\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0130 - mean_squared_error: 0.0261\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0100 - mean_squared_error: 0.0199\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0168 - mean_squared_error: 0.0336\n","Epsiode 114 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0108 - mean_squared_error: 0.0217\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0105 - mean_squared_error: 0.0210\n","Copying weights from main to target\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0143 - mean_squared_error: 0.0286\n","Epsiode 115 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0132 - mean_squared_error: 0.0264\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0129 - mean_squared_error: 0.0259\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0061 - mean_squared_error: 0.0122\n","Epsiode 116 statistics: \n","Reward obtained:  9.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0066 - mean_squared_error: 0.0133\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0062 - mean_squared_error: 0.0124\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0191 - mean_squared_error: 0.0383\n","Epsiode 117 statistics: \n","Reward obtained:  9.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0119 - mean_squared_error: 0.0239\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0135 - mean_squared_error: 0.0271\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0122 - mean_squared_error: 0.0245\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0175 - mean_squared_error: 0.0351\n","Epsiode 118 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0167 - mean_squared_error: 0.0333\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0137 - mean_squared_error: 0.0274\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0087 - mean_squared_error: 0.0174\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0105 - mean_squared_error: 0.0210\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0159 - mean_squared_error: 0.0318\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0123 - mean_squared_error: 0.0247\n","Epsiode 119 statistics: \n","Reward obtained:  22.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0105 - mean_squared_error: 0.0210\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0124 - mean_squared_error: 0.0248\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0144 - mean_squared_error: 0.0288\n","Epsiode 120 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0169 - mean_squared_error: 0.0338\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0097 - mean_squared_error: 0.0194\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_squared_error: 0.0170\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0111 - mean_squared_error: 0.0222\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0123 - mean_squared_error: 0.0245\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0076 - mean_squared_error: 0.0153\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0105 - mean_squared_error: 0.0210\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0057 - mean_squared_error: 0.0113\n","Epsiode 121 statistics: \n","Reward obtained:  29.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0146 - mean_squared_error: 0.0292\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0172 - mean_squared_error: 0.0346\n","Copying weights from main to target\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0167 - mean_squared_error: 0.0335\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0116 - mean_squared_error: 0.0232\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0170 - mean_squared_error: 0.0341\n","Epsiode 122 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0116 - mean_squared_error: 0.0233\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0141 - mean_squared_error: 0.0283\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0070 - mean_squared_error: 0.0140\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0060 - mean_squared_error: 0.0120\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0128 - mean_squared_error: 0.0259\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0074 - mean_squared_error: 0.0147\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0119 - mean_squared_error: 0.0237\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0098 - mean_squared_error: 0.0197\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0218 - mean_squared_error: 0.0436\n","Epsiode 123 statistics: \n","Reward obtained:  35.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0133 - mean_squared_error: 0.0266\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0120 - mean_squared_error: 0.0242\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0154 - mean_squared_error: 0.0309\n","Epsiode 124 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0116 - mean_squared_error: 0.0232\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0090 - mean_squared_error: 0.0179\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0113 - mean_squared_error: 0.0225\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0096 - mean_squared_error: 0.0193\n","Epsiode 125 statistics: \n","Reward obtained:  13.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0121 - mean_squared_error: 0.0241\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0071 - mean_squared_error: 0.0141\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0105 - mean_squared_error: 0.0209\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0129 - mean_squared_error: 0.0258\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0218 - mean_squared_error: 0.0436\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0186 - mean_squared_error: 0.0373\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0086 - mean_squared_error: 0.0172\n","Epsiode 126 statistics: \n","Reward obtained:  24.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0109 - mean_squared_error: 0.0218\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0103 - mean_squared_error: 0.0206\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0105 - mean_squared_error: 0.0210\n","Copying weights from main to target\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0115 - mean_squared_error: 0.0229\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0084 - mean_squared_error: 0.0168\n","Epsiode 127 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0091 - mean_squared_error: 0.0183\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0094 - mean_squared_error: 0.0188\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0078 - mean_squared_error: 0.0155\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0127 - mean_squared_error: 0.0254\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0136 - mean_squared_error: 0.0273\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0126 - mean_squared_error: 0.0253\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0109 - mean_squared_error: 0.0218\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0103 - mean_squared_error: 0.0206\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0128 - mean_squared_error: 0.0257\n","Epsiode 128 statistics: \n","Reward obtained:  32.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0090 - mean_squared_error: 0.0181\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0085 - mean_squared_error: 0.0170\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0085 - mean_squared_error: 0.0170\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0059 - mean_squared_error: 0.0118\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0096 - mean_squared_error: 0.0192\n","Epsiode 129 statistics: \n","Reward obtained:  17.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0119 - mean_squared_error: 0.0237\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0133 - mean_squared_error: 0.0265\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0061 - mean_squared_error: 0.0123\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0065 - mean_squared_error: 0.0130\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0103 - mean_squared_error: 0.0207\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0080 - mean_squared_error: 0.0159\n","Epsiode 130 statistics: \n","Reward obtained:  19.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0073 - mean_squared_error: 0.0146\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0073 - mean_squared_error: 0.0146\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0066 - mean_squared_error: 0.0131\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0078 - mean_squared_error: 0.0155\n","Epsiode 131 statistics: \n","Reward obtained:  14.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0103 - mean_squared_error: 0.0206\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0106 - mean_squared_error: 0.0213\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0069 - mean_squared_error: 0.0138\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0092 - mean_squared_error: 0.0183\n","Epsiode 132 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0143 - mean_squared_error: 0.0290\n","Copying weights from main to target\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0095 - mean_squared_error: 0.0190\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0126 - mean_squared_error: 0.0252\n","Epsiode 133 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0053 - mean_squared_error: 0.0106\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0088 - mean_squared_error: 0.0176\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0092 - mean_squared_error: 0.0183\n","Epsiode 134 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0113 - mean_squared_error: 0.0226\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0165 - mean_squared_error: 0.0331\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0109 - mean_squared_error: 0.0217\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0081 - mean_squared_error: 0.0162\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0056 - mean_squared_error: 0.0112\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0092 - mean_squared_error: 0.0185\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0051 - mean_squared_error: 0.0102\n","Epsiode 135 statistics: \n","Reward obtained:  26.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0077 - mean_squared_error: 0.0153\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0090 - mean_squared_error: 0.0179\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0103 - mean_squared_error: 0.0206\n","Epsiode 136 statistics: \n","Reward obtained:  9.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0124 - mean_squared_error: 0.0248\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0086 - mean_squared_error: 0.0171\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0067 - mean_squared_error: 0.0134\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0089 - mean_squared_error: 0.0177\n","Epsiode 137 statistics: \n","Reward obtained:  12.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0137 - mean_squared_error: 0.0273\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0047 - mean_squared_error: 0.0094\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0068 - mean_squared_error: 0.0135\n","Epsiode 138 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0087 - mean_squared_error: 0.0173\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0096 - mean_squared_error: 0.0192\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0138 - mean_squared_error: 0.0276\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0083 - mean_squared_error: 0.0166\n","Epsiode 139 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0163 - mean_squared_error: 0.0327\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0084 - mean_squared_error: 0.0167\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0057 - mean_squared_error: 0.0115\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0094 - mean_squared_error: 0.0188\n","Copying weights from main to target\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0068 - mean_squared_error: 0.0136\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0089 - mean_squared_error: 0.0178\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0062 - mean_squared_error: 0.0125\n","16/16 [==============================] - 0s 23ms/step - loss: 0.0057 - mean_squared_error: 0.0114\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0144 - mean_squared_error: 0.0288\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0102 - mean_squared_error: 0.0204\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0063 - mean_squared_error: 0.0126\n","Epsiode 140 statistics: \n","Reward obtained:  40.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0114 - mean_squared_error: 0.0227\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0080 - mean_squared_error: 0.0160\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0144 - mean_squared_error: 0.0288\n","Epsiode 141 statistics: \n","Reward obtained:  11.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0103 - mean_squared_error: 0.0207\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0127 - mean_squared_error: 0.0255\n","16/16 [==============================] - 0s 27ms/step - loss: 0.0103 - mean_squared_error: 0.0205\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0052 - mean_squared_error: 0.0104\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0076 - mean_squared_error: 0.0152\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0078 - mean_squared_error: 0.0156\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0061 - mean_squared_error: 0.0121\n","Epsiode 142 statistics: \n","Reward obtained:  26.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0142 - mean_squared_error: 0.0283\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0056 - mean_squared_error: 0.0112\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0074 - mean_squared_error: 0.0148\n","Epsiode 143 statistics: \n","Reward obtained:  10.0\n","16/16 [==============================] - 0s 24ms/step - loss: 0.0086 - mean_squared_error: 0.0171\n","16/16 [==============================] - 0s 26ms/step - loss: 0.0073 - mean_squared_error: 0.0145\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0053 - mean_squared_error: 0.0107\n","Epsiode 144 statistics: \n","Reward obtained:  12.0\n","16/16 [==============================] - 0s 25ms/step - loss: 0.0111 - mean_squared_error: 0.0222\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"id":"Xaa6BFxffNi4","executionInfo":{"status":"error","timestamp":1618410445947,"user_tz":240,"elapsed":324,"user":{"displayName":"Pranjal Gupta","photoUrl":"","userId":"09893061847420234743"}},"outputId":"8475993b-10e2-4c28-bf9a-c73b91dde73d"},"source":["plt.plot(agent.all_rewards)\n","plt.title('Rewards obtained in each episode')\n","plt.xlabel('episodes')\n","plt.ylabel('reward')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b9f0e346c7e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards obtained in each episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'episodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}]},{"cell_type":"code","metadata":{"id":"uDiZZv9Gx5r0"},"source":[""],"execution_count":null,"outputs":[]}]}